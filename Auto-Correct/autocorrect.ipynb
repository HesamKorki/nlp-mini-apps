{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0381002a92f4eb065232e75d360dbf83f6d8804854f4152103c42246c6ee94967",
   "display_name": "Python 3.8.3 64-bit ('env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "381002a92f4eb065232e75d360dbf83f6d8804854f4152103c42246c6ee94967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Auto Correct\n",
    "\n",
    "In this notebook I want to implement a fully working auto correct system which is simple yet functional. To elaborate, the system would correct words that are 1 and 2 edit distances away. \n",
    "An edit could consist of one of the following:\n",
    "- Delete: \"mangoz\" -> \"mango\", \"mngoz\", ...\n",
    "\n",
    "- Switch: \"atm\": \"tam\", \"amt\", ...\n",
    "\n",
    "- Replace: \"rat\": \"hat\", \"mat\", \"rap\", ...\n",
    "\n",
    "- Insert: \"pla\": \"play\", \"pela\", ...\n",
    "\n",
    "# Data PreProcessing\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import re\r\n",
    "from collections import Counter\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is to define a __process_data__ function that would read the corpus --*shakespeare.txt*-- and build a vocabulary based on that."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def process_data(file_name):\r\n",
    "    \"\"\"\r\n",
    "    Input: \r\n",
    "        A file_name which is found in your current directory. You            just have to read it in. \r\n",
    "    Output: \r\n",
    "        words: a list containing all the  words in the corpus (text           file you read) in lower case. \r\n",
    "    \"\"\"\r\n",
    "    with open(file_name) as f:\r\n",
    "        raw = f.read()\r\n",
    "    low_data = raw.lower()\r\n",
    "    words = re.findall(r\"\\w+\", low_data)\r\n",
    "    \r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "words = process_data(\"shakespeare.txt\")\r\n",
    "vocab = set(words)\r\n",
    "print(f\"There are {len(vocab)} unique words. \\nHere's 10 random samples from the vocabulary:\")\r\n",
    "random.sample(vocab, 10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 6116 unique words. \n",
      "Here's 10 random samples from the vocabulary:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\hesam\\AppData\\Local\\Temp/ipykernel_20600/4094430650.py:4: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random.sample(vocab, 10)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['sets',\n",
       " 'sighs',\n",
       " 'cox',\n",
       " 'awards',\n",
       " 'dyed',\n",
       " 'physic',\n",
       " 'terrors',\n",
       " 'reference',\n",
       " 'abhor',\n",
       " 'bind']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need a dictionary with words as its keys and the value of each is the number of times that word appear in our corpus. A word_frequency dictionary in essence. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_count(word_l):\r\n",
    "    '''\r\n",
    "    Input:\r\n",
    "        word_l: a set of words representing the corpus. \r\n",
    "    Output:\r\n",
    "        word_count_dict: The wordcount dictionary where key is the           word and value is its frequency.\r\n",
    "    '''\r\n",
    "    \r\n",
    "    word_count_dict = {} \r\n",
    "    for word in word_l:\r\n",
    "        \r\n",
    "        if word_count_dict.get(word, False):\r\n",
    "            word_count_dict[word] += 1\r\n",
    "        else:\r\n",
    "            word_count_dict[word] = 1\r\n",
    "\r\n",
    "    return word_count_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "word_frequency = get_count(words)\r\n",
    "sample_words = random.sample(list(word_frequency),10)\r\n",
    "{k: word_frequency[k] for k in sample_words} ## 10 random sample from word_frequency dictionary"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'travel': 7,\n",
       " 'perceived': 2,\n",
       " '54': 1,\n",
       " 'except': 2,\n",
       " 'eclipses': 2,\n",
       " 'carrier': 1,\n",
       " 'sharpness': 1,\n",
       " 'punishment': 1,\n",
       " 'guards': 1,\n",
       " 'curstness': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_probs(word_count_dict):\r\n",
    "    '''\r\n",
    "    Input:\r\n",
    "        word_count_dict: The wordcount dictionary where key is the           word and value is its frequency.\r\n",
    "    Output:\r\n",
    "        probs: A dictionary where keys are the words and the values          are the probability that a word will occur. \r\n",
    "    '''\r\n",
    "    \r\n",
    "    total_words = sum(word_count_dict.values())\r\n",
    "    probs = {word: word_count_dict[word]/total_words for word in word_count_dict.keys()}\r\n",
    "    \r\n",
    "    return probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "probs = get_probs(word_frequency)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# String Edits\r\n",
    "\r\n",
    "* `delete_letter`: given a word, it returns all the possible strings that have **one character removed**. \r\n",
    "* `switch_letter`: given a word, it returns all the possible strings that have **two adjacent letters switched**.\r\n",
    "* `replace_letter`: given a word, it returns all the possible strings that have **one character replaced by another different letter**.\r\n",
    "* `insert_letter`: given a word, it returns all the possible strings that have an **additional character inserted**. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def delete_letter(word, verbose=False):\r\n",
    "    '''\r\n",
    "    Input:\r\n",
    "        word: the string/word for which you will generate all                possible words in the vocabulary which have 1 missing                character\r\n",
    "    Output:\r\n",
    "        delete_l: a list of all possible strings obtained by                 deleting 1 character from word\r\n",
    "    '''\r\n",
    "     \r\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\r\n",
    "    delete_l = [L+R[1:] for L,R in split_l if R]\r\n",
    "\r\n",
    "\r\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\r\n",
    "\r\n",
    "    return delete_l"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "delete_word_l = delete_letter(word=\"brand\",\r\n",
    "                        verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input word brand, \n",
      "split_l = [('', 'brand'), ('b', 'rand'), ('br', 'and'), ('bra', 'nd'), ('bran', 'd')], \n",
      "delete_l = ['rand', 'band', 'brnd', 'brad', 'bran']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def switch_letter(word, verbose=False):\r\n",
    "    '''\r\n",
    "    Input:\r\n",
    "        word: input string\r\n",
    "     Output:\r\n",
    "        switches: a list of all possible strings with one adjacent           charater switched\r\n",
    "    ''' \r\n",
    "    \r\n",
    "    \r\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\r\n",
    "    switch_l = [L[:-1] + R[0] + L[-1] + R[1:] for L,R in split_l if R and L]\r\n",
    "\r\n",
    "    \r\n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \r\n",
    "\r\n",
    "    return switch_l"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "switch_word_l = switch_letter(word=\"eta\",\r\n",
    "                         verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input word = eta \n",
      "split_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \n",
      "switch_l = ['tea', 'eat']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def replace_letter(word, verbose=False):\r\n",
    "    '''\r\n",
    "    Input:\r\n",
    "        word: the input string/word \r\n",
    "    Output:\r\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \r\n",
    "    ''' \r\n",
    "    \r\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\r\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\r\n",
    "    replace_l = [L + let + R[1:] for L,R in split_l for let in letters]\r\n",
    "    replace_set = set(replace_l)\r\n",
    "    replace_set.remove(word)\r\n",
    "\r\n",
    "    \r\n",
    "    # turn the set back into a list and sort it, for easier viewing\r\n",
    "    replace_l = sorted(list(replace_set))\r\n",
    "    \r\n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \r\n",
    "    \r\n",
    "    return replace_l"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "replace_l = replace_letter(word='can',\r\n",
    "                              verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input word = can \n",
      "split_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \n",
      "replace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def insert_letter(word, verbose=False):\r\n",
    "    '''\r\n",
    "    Input:\r\n",
    "        word: the input string/word \r\n",
    "    Output:\r\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\r\n",
    "    ''' \r\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\r\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word)+1)]\r\n",
    "    insert_l = [L + let + R for L,R in split_l for let in letters]\r\n",
    "\r\n",
    "\r\n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\r\n",
    "    \r\n",
    "    return insert_l"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "insert_l = insert_letter('at', True)\r\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input word at \n",
      "split_l = [('', 'at'), ('a', 't'), ('at', '')] \n",
      "insert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combining Edits\r\n",
    "We need two functions that wrap up all the possible single and double edits. `edit_one_letter()` and `edit_two_letters()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def edit_one_letter(word, allow_switches = True):\r\n",
    "    \"\"\"\r\n",
    "    Input:\r\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\r\n",
    "    Output:\r\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\r\n",
    "    \"\"\"\r\n",
    "    edit_one_set = set()\r\n",
    "    edit_one_set.update(delete_letter(word))\r\n",
    "    edit_one_set.update(insert_letter(word))\r\n",
    "    edit_one_set.update(replace_letter(word))\r\n",
    "    if allow_switches:\r\n",
    "        edit_one_set.update(switch_letter(word))\r\n",
    "    \r\n",
    "\r\n",
    "    return edit_one_set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "tmp_word = \"at\"\r\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\r\n",
    "# turn this into a list to sort it, in order to view it\r\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\r\n",
    "\r\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\r\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\r\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input word at \n",
      "edit_one_l \n",
      "['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n",
      "\n",
      "The type of the returned object should be a set <class 'set'>\n",
      "Number of outputs from edit_one_letter('at') is 129\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we want to get all the possible strings that are two edits away from a given word. This can be done by performing the `edit_one_letter` method on the set of one edits of a given word."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def edit_two_letters(word, allow_switches = True):\r\n",
    "    '''\r\n",
    "    Input:\r\n",
    "        word: the input string/word \r\n",
    "    Output:\r\n",
    "        edit_two_set: a set of strings with all possible two edits\r\n",
    "    '''\r\n",
    "    \r\n",
    "    edit_two_set = set()\r\n",
    "    \r\n",
    "    edit_one = edit_one_letter(word,allow_switches)\r\n",
    "    for w in edit_one:\r\n",
    "        if w:\r\n",
    "            edit_two = edit_one_letter(w,allow_switches)\r\n",
    "            edit_two_set.update(edit_two)\r\n",
    "\r\n",
    "    \r\n",
    "    return edit_two_set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\r\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\r\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\r\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\r\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\r\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\r\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of strings with edit distance of two: 2654\n",
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']\n",
      "Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Suggest Spelling\r\n",
    "At this point, we have a set of possible 2 edits of the word. We need to find the most probable spelling from them. A function called `get_corrections` needs to be implemented that for any given word, it would return a list of edited words and their respective probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\r\n",
    "    '''\r\n",
    "    Input: \r\n",
    "        word: a user entered string to check for suggestions\r\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\r\n",
    "        vocab: a set containing all the vocabulary\r\n",
    "        n: number of possible word corrections you want returned in the dictionary\r\n",
    "    Output: \r\n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\r\n",
    "    '''\r\n",
    "    \r\n",
    "    suggestions = []\r\n",
    "    n_best = []\r\n",
    "    \r\n",
    "\r\n",
    "    suggestions = list((word in vocab and word) or edit_one_letter(word).intersection(vocab) or edit_two_letters(word).intersection(vocab))\r\n",
    "    n_best = [[s,probs[s]] for s in list(reversed(suggestions))]\r\n",
    "\r\n",
    "    \r\n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\r\n",
    "\r\n",
    "    return n_best"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Test your implementation - feel free to try other words in my word\r\n",
    "my_word = 'dys' \r\n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\r\n",
    "for i, word_prob in enumerate(tmp_corrections):\r\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\r\n",
    "\r\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\r\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "entered word =  dys \n",
      "suggestions =  ['days', 'dye']\n",
      "word 0: dye, probability 0.000019\n",
      "word 1: days, probability 0.000410\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can try different typos to see how well this simple auto correct works:\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "SAMPLE_WORD = \"dys\"\r\n",
    "corrected = get_corrections(SAMPLE_WORD, probs, vocab, 5, verbose=True)\r\n",
    "max_prob = 0\r\n",
    "ind = 0\r\n",
    "for i, sug in enumerate(corrected):\r\n",
    "    if sug[1] > max_prob:\r\n",
    "        ind = i\r\n",
    "        max_prob = sug[1]\r\n",
    "if corrected:\r\n",
    "    print(\"The most probable correction is: \", corrected[ind][0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "entered word =  dys \n",
      "suggestions =  ['days', 'dye']\n",
      "The most probable correction is:  days\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reference \n",
    "NLP Specialization course from Stanford university"
   ],
   "metadata": {}
  }
 ]
}